{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2db407a",
   "metadata": {},
   "source": [
    "## Question 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78372dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new': 1, 'home': 1, 'ha': 1, 'been': 1, 'sale': 1, 'top': 1, 'forecast': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from porter import *\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "stopwords = ['the', 'a', 'an', 'of', 'behind', 'under', 'there', 'in', 'on']\n",
    "\n",
    "def preprocess(text) :\n",
    "    # Separate words\n",
    "    words = re.split(r'\\W+', text)\n",
    "    # Turn into lowercase\n",
    "    words_lowercase = (w.lower() for w in words)\n",
    "    # Remove stopwords\n",
    "    words_nostopword = (w for w in words_lowercase if w not in stopwords)\n",
    "    # Apply stemming\n",
    "    words_stemmed = (stem(w.lower()) for w in words_nostopword)\n",
    "    # Delete stopwords\n",
    "    return dict(Counter(words_stemmed))\n",
    "\n",
    "\n",
    "doc1 = \"The new home has been saled on top of forecasts\"\n",
    "doc2 = \"The home sales rise in July\"\n",
    "doc3 = \"There is an increase in home sales in July\"\n",
    "doc4 = \"July encounter a new home sales rise\"\n",
    "\n",
    "corpus = [doc1,doc2,doc3,doc4]\n",
    "\n",
    "preprocess(doc1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef8ffc4",
   "metadata": {},
   "source": [
    "# Question 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77b9391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(corp) :\n",
    "    \"\"\"Creates an index file for each documents in the corpus\"\"\"\n",
    "    result = dict()\n",
    "    for i,doc in enumerate(corp) :\n",
    "        result[i] = preprocess(doc)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "204763aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'new': 1,\n",
       "  'home': 1,\n",
       "  'ha': 1,\n",
       "  'been': 1,\n",
       "  'sale': 1,\n",
       "  'top': 1,\n",
       "  'forecast': 1},\n",
       " 1: {'home': 1, 'sale': 1, 'rise': 1, 'juli': 1},\n",
       " 2: {'is': 1, 'increas': 1, 'home': 1, 'sale': 1, 'juli': 1},\n",
       " 3: {'juli': 1, 'encount': 1, 'new': 1, 'home': 1, 'sale': 1, 'rise': 1}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_index(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f88b3377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new': {0: 1, 3: 1},\n",
       " 'home': {0: 1, 1: 1, 2: 1, 3: 1},\n",
       " 'ha': {0: 1},\n",
       " 'been': {0: 1},\n",
       " 'sale': {0: 1, 1: 1, 2: 1, 3: 1},\n",
       " 'top': {0: 1},\n",
       " 'forecast': {0: 1},\n",
       " 'rise': {1: 1, 3: 1},\n",
       " 'juli': {1: 1, 2: 1, 3: 1},\n",
       " 'is': {2: 1},\n",
       " 'increas': {2: 1},\n",
       " 'encount': {3: 1}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_index_inverse(corpus):\n",
    "    d = {}\n",
    "    ind = create_index(corpus)\n",
    "    for k1,v1 in ind.items():\n",
    "        for k2,v2 in ind[k1].items():\n",
    "            if k2 in d.keys():\n",
    "                d[k2][k1] = v2\n",
    "            else:\n",
    "                d[k2] = {k1 : v2}\n",
    "    return d\n",
    "\n",
    "create_index_inverse(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be5ca714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new': {0: 0.6931471805599453, 3: 0.6931471805599453},\n",
       " 'home': {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0},\n",
       " 'ha': {0: 1.3862943611198906},\n",
       " 'been': {0: 1.3862943611198906},\n",
       " 'sale': {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0},\n",
       " 'top': {0: 1.3862943611198906},\n",
       " 'forecast': {0: 1.3862943611198906},\n",
       " 'rise': {1: 0.6931471805599453, 3: 0.6931471805599453},\n",
       " 'juli': {1: 0.28768207245178085,\n",
       "  2: 0.28768207245178085,\n",
       "  3: 0.28768207245178085},\n",
       " 'is': {2: 1.3862943611198906},\n",
       " 'increas': {2: 1.3862943611198906},\n",
       " 'encount': {3: 1.3862943611198906}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_index_inverse_tfidf(corp) :\n",
    "    \"\"\"Creates an index file for each documents in the corpus\"\"\"\n",
    "    index = create_index_inverse(corp)\n",
    "    for term in index :\n",
    "        for doc in index[term] :\n",
    "            index[term][doc] = index[term][doc] * np.log(len(corp)/len(index[term]))\n",
    "    return index\n",
    "    \n",
    "create_index_inverse_tfidf(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceee8ae",
   "metadata": {},
   "source": [
    "# EXERCICE 2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9137791f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ir_datasets in /Users/allaa/miniconda3/lib/python3.10/site-packages (0.5.4)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from ir_datasets) (4.11.1)\n",
      "Requirement already satisfied: requests>=2.22.0 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from ir_datasets) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from ir_datasets) (1.23.5)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from ir_datasets) (0.2.3)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from ir_datasets) (0.2.5)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from ir_datasets) (0.2.2)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from ir_datasets) (0.1.9)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from ir_datasets) (3.2.0.post0)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from ir_datasets) (4.9.1)\n",
      "Requirement already satisfied: tqdm>=4.38.0 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from ir_datasets) (4.64.1)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from ir_datasets) (0.1.5)\n",
      "Requirement already satisfied: lz4>=3.1.1 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from ir_datasets) (4.3.2)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from ir_datasets) (2.6)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from ir_datasets) (5.4.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.3.2.post1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from requests>=2.22.0->ir_datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from requests>=2.22.0->ir_datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from requests>=2.22.0->ir_datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from requests>=2.22.0->ir_datasets) (2022.12.7)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /Users/allaa/miniconda3/lib/python3.10/site-packages (from trec-car-tools>=2.5.4->ir_datasets) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade ir_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a0c11065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [starting] opening zip file\n",
      "[INFO] [starting] https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scifact.zip\n",
      "[INFO] [finished] https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scifact.zip: [00:04] [2.82MB] [698kB/s]\n",
      "[INFO] [finished] opening zip file [4.16s]                                                                \n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "\n",
    "dataset = ir_datasets.load('beir/scifact')\n",
    "for query in dataset.queries_iter():\n",
    "    query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "886d2119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq as hp\n",
    "\n",
    "def taat(query,index,k) :\n",
    "    h = dict()\n",
    "    \n",
    "    vocabulary = index.keys()\n",
    "    query_words = preprocess(query).keys()\n",
    "    workspace = {word:index[word] for word in query_words}\n",
    "    for term in workspace :\n",
    "        for doc in workspace[term] :\n",
    "            if doc in h : h[doc] += workspace[term][doc]\n",
    "            else : h[doc] = workspace[term][doc]\n",
    "        \n",
    "    top_K = heapq.nlargest(k, h.items(), key=lambda x: x[1])\n",
    "    print(top_K)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64742168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 1.3862943611198906)]\n"
     ]
    }
   ],
   "source": [
    "taat(\"is\",create_index_inverse_tfidf(corpus), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f0e5f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [doc[1] for doc in dataset.docs_iter()]\n",
    "index = create_index_inverse_tfidf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ddb45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
